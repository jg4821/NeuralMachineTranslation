{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import io\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "import itertools\n",
    "import glob\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "from sacrebleu import corpus_bleu\n",
    "import sacrebleu\n",
    "import math\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "words_to_load = 100000\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "UNK_token = 3\n",
    "LR_RATE = 0.001\n",
    "MAX_LENGTH = 40\n",
    "hidden_size = 100\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = '/scratch/wz1218'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocess Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"<pad>\", 3: \"<unk>\"}\n",
    "        self.n_words = 4  # Count SOS, EOS, pad and unk\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalizeZh(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(\"\\s+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filterPair(p):\n",
    "#     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "#         len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# def filterPairs(pairs):\n",
    "#     return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    filtered = []\n",
    "    for i in p:\n",
    "        filtered.append(' '.join(i.split()[:MAX_LENGTH]))\n",
    "    return filtered\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [filterPair(pair) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(dataset, lang1, lang2):\n",
    "    chinese = add+'/iwslt-zh-en/{}.tok.{}'.format(dataset, lang1)\n",
    "    english = add+'/iwslt-zh-en/{}.tok.{}'.format(dataset, lang2)\n",
    "\n",
    "    chinese_lines = open(chinese, encoding='utf-8').read().strip().split('\\n')\n",
    "    english_lines = open(english, encoding='utf-8').read().strip().split('\\n')\n",
    "    length = len(chinese_lines)\n",
    "\n",
    "    pairs = [[normalizeZh(chinese_lines[i]), normalizeString(english_lines[i])] for i in range(length)]\n",
    "    pairs = filterPairs(pairs)\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_lang, train_output_lang, train_pairs = readLangs('train', 'zh', 'en')\n",
    "val_input_lang, val_output_lang, val_pairs = readLangs('dev', 'zh', 'en')\n",
    "test_input_lang, test_output_lang, test_pairs = readLangs('test', 'zh', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213376"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pairs)  # 6621 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1261"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_pairs)  # 39 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Loader__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_lang, output_lang, pairs):\n",
    "        \"\"\"\n",
    "        @param data_list_1: list of sentence 1 tokens \n",
    "        @param data_list_2: list of sentence 2 tokens\n",
    "        @param target_list: list of review targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        input_sentence = self.pairs[key][0]\n",
    "        input_indexes = [self.input_lang.word2index[word] if word in self.input_lang.word2index else UNK_token for word in input_sentence.split(' ')]\n",
    "        input_indexes.append(EOS_token)\n",
    "        input_length = len(input_indexes)\n",
    "\n",
    "        output_sentence = self.pairs[key][1]\n",
    "        output_indexes = [self.output_lang.word2index[word] if word in self.output_lang.word2index else UNK_token for word in output_sentence.split(' ')]\n",
    "        output_indexes.append(EOS_token)\n",
    "        output_length = len(output_indexes)\n",
    "        return [input_indexes, input_length, output_indexes, output_length]\n",
    "\n",
    "    \n",
    "def NMTDataset_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    input_ls = []\n",
    "    output_ls = []\n",
    "    input_length_ls = []\n",
    "    output_length_ls = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        input_length_ls.append(datum[1])\n",
    "        output_length_ls.append(datum[3])\n",
    "    \n",
    "    #find max length in each batch\n",
    "    max_input = sorted(input_length_ls)[-1]\n",
    "    max_output = sorted(output_length_ls)[-1]\n",
    "    \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_input = np.pad(np.array(datum[0]), \n",
    "                                  pad_width=((0,max_input-datum[1])), \n",
    "                                  mode=\"constant\", constant_values=2).tolist()\n",
    "        padded_vec_output = np.pad(np.array(datum[2]), \n",
    "                                   pad_width=((0,max_output-datum[3])), \n",
    "                                   mode=\"constant\", constant_values=2).tolist()\n",
    "        input_ls.append(padded_vec_input)\n",
    "        output_ls.append(padded_vec_output)\n",
    "    return [torch.tensor(torch.from_numpy(np.array(input_ls)), device=device), \n",
    "            torch.tensor(input_length_ls, device=device), \n",
    "            torch.tensor(torch.from_numpy(np.array(output_ls)), device=device), \n",
    "            torch.tensor(output_length_ls, device=device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataloader\n",
    "train_dataset = NMTDataset(train_input_lang, train_output_lang, train_pairs)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=NMTDataset_collate_func,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "val_dataset = NMTDataset(train_input_lang, train_output_lang, val_pairs)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         collate_fn=NMTDataset_collate_func,\n",
    "                                         shuffle=False,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   46,  2944,  1987,  ...,     2,     2,     2],\n",
       "         [   46,     6,  3298,  ...,     2,     2,     2],\n",
       "         [  482, 19797, 53846,  ...,     2,     2,     2],\n",
       "         ...,\n",
       "         [  682,   482,   727,  ...,     2,     2,     2],\n",
       "         [  107,  5581,  8922,  ...,     2,     2,     2],\n",
       "         [   46,   168,   197,  ...,     2,     2,     2]], device='cuda:0'),\n",
       " tensor([16, 15, 21,  9, 17, 13,  7,  5, 16, 28, 11, 18, 18, 17,  8, 18, 13,  9,\n",
       "          6,  7, 10, 16, 13, 36, 13,  4, 23, 19, 23, 15, 25, 28],\n",
       "        device='cuda:0'),\n",
       " tensor([[   87,    45,   412,    45,   891,  3708,   192,   182,  1165,    20,\n",
       "              6,   352,    22,  3554,     5,   183,  2136,    44,     1,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  183,  2078,   412,  3133,    20, 27045,  1098,   116,   282,   808,\n",
       "          15208,  3483,    44,     1,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  112,   412,   139,   108,  3614,   116,   282,   311,   425,   412,\n",
       "           3279,   310,   185,     6,  1098,   127,  3081,  2597,    44,     1,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  160,     6,  5286,    89,  3626,   312,   160,   183,  2078, 20879,\n",
       "             44,     1,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   45,   423,    24,   248,   764,   176,    68,  2080,   135,    45,\n",
       "            627,   102,    61,   183,  2078,   412,   140,   140,  1134,    44,\n",
       "              1,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  160,    52,   279,   289,    20,   139,   259,   908,   389,   160,\n",
       "            284,   159,    44,     1,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  139,  1165,    61,    45,   515,   393,   257,    44,     1,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  139,   259,   908,    44,     1,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   52,   102,    45,   412,   429,    87,     6,  5286,  1335,   339,\n",
       "           1311,    30,   236,    68, 13904,    85,  3616,    20,   289,    20,\n",
       "            908,    44,     1,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  164,    85,     6,   971,   675,   650,    45,  8668,   169,   139,\n",
       "           2213,    20, 43564,   183,  2117,  2623,  1019,   412,   457,  1010,\n",
       "           2577,    20,   189,  2723,  1258,    20,   139,  4948,   908,    44,\n",
       "              1],\n",
       "         [   68,   412,     6,   175,   309,    48,  1728,   627,   189,  1447,\n",
       "             44,     1,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [ 1765,   913,    48,  1335,   139,   196,  6402,   164,    61,   457,\n",
       "            182,   421,  9918,    37,    48,   528,    50,    44,     1,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   48,   421,  1362,   250,  2876,     5,  3699,  3934,   164,    68,\n",
       "            421,  3411,    48,   528,   495,   106,  1588,    44,     1,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [    6,   908,   412,     5,   139,  2136,   997,   125,    22,    19,\n",
       "            402,     5,   182,   808,   765,   373,    44,     1,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   68,   412, 22906,     5,  4649,   135,  3115,   478,     5,  4233,\n",
       "             44,     1,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   48,    74,   411,    48,   528,  3432,   250,   484,     6,  1579,\n",
       "              6,  1340,    30,   250,  1471,    44,     1,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   54,   376,    20,   376,     6,   908,   421,  3710,   189,  6098,\n",
       "             85,   139,   665,   185,  5286,   528, 10948,    44,     1,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   48,  1214,  2781,   176,   205,   411,    88,    19,    44,     1,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  528,    48,   499,  2036,   198,     1,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  378,   205,   764,    37,    48,   262,   198,     1,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   48,   528,  4519,   135,  1006,   908,   412,    37,    48,  1344,\n",
       "             20,   189,    44,     1,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   45,   412,   140,  3607,    20,   680,   192,     5,   139,   835,\n",
       "             37,   739,   412,  9100,    30,  3168,   528, 14270,    44,     1,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  183,  2257,   412,   302,  1620,  3225,    85,   282,   376,    44,\n",
       "              1,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  139,  3024, 21693,    54,   139,  3846, 12583,    22,  1311,   284,\n",
       "           5733,    61,   282,  1973,   183,  6548,   289,    20,   908,    30,\n",
       "             85,    61,   284,   412,     3,    14,   282,  2078,    44,     1,\n",
       "              2],\n",
       "         [  135,   183,  1447,  1972,  2133,   139,  1579,    44,     1,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  112,   892,    38,    44,     1,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  892, 15944,    95,   650,   783,   175,    20,  1550,   250,  2136,\n",
       "            147,   139,   908,    85,  3616,    30,  1518,     5,   250,  2426,\n",
       "             44,     1,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   30,   183,  2078,    61,    24,    25,  2597,   284,   412,     6,\n",
       "            260,    62,     5,   282,   835,    20,  6958,   302,   739,    44,\n",
       "              1,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [  112,   412,   457,  1565,    61,   282,  1394,   421,  6958,   302,\n",
       "            739,  3032,   282,  3168,   679,     6,  5286,   679,     6,  2510,\n",
       "             44,     1,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [   20,  2597,   112,   412,   360,  2481,     5,    65, 16815,   282,\n",
       "           1394,    44,     1,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2],\n",
       "         [ 2009,  5286,   650,    45,   891,   112,   528,   862,    45,   421,\n",
       "            146,   164,  7964,    14,   250,     4,    30,  1214,   499,  4519,\n",
       "             30,    65,   399,   139,   819,    44,     1,     2,     2,     2,\n",
       "              2],\n",
       "         [   45,   421,   137,    20,  2648,   135,   183,  2078,   284,   421,\n",
       "            709,   160,   707,   183,  1973,    52,   279,  4918,   518,    52,\n",
       "           1184,     5,   496,     4,    44,     1,     2,     2,     2,     2,\n",
       "              2]], device='cuda:0'),\n",
       " tensor([19, 14, 20, 12, 21, 14,  9,  5, 23, 31, 12, 19, 19, 18, 12, 17, 19, 10,\n",
       "          6,  8, 14, 20, 11, 30,  9,  5, 22, 21, 22, 13, 27, 26],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "#     print(i)\n",
    "#     print(i[2])\n",
    "#     for ind in i[2]:\n",
    "#         for token in ind:\n",
    "#             print(token)\n",
    "#             print(train_output_lang.index2word[token.item()])\n",
    "#     for ind in i[2]:\n",
    "#         print(' '.join(train_output_lang.index2word[token.item()] for token in ind))\n",
    "#     print([train_output_lang.index2word[token.item()] for ind in i[2] for token in ind])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(ft_path, words_to_load):\n",
    "    fin = io.open(ft_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    vocab_size = words_to_load + 4\n",
    "    embedding_dim = d\n",
    "\n",
    "    embedding_mat = np.zeros((vocab_size, embedding_dim))\n",
    "    token2id = {}\n",
    "    id2token = {}\n",
    "    all_tokens = ['SOS', 'EOS', '<unk>', '<pad>']\n",
    "\n",
    "    for i, line in enumerate(fin):\n",
    "        if i >= words_to_load:\n",
    "            break\n",
    "        s = line.rstrip().split(' ')\n",
    "        embedding_mat[i+4, :] = np.asarray(s[1:])\n",
    "        token2id[s[0]] = i+4\n",
    "        id2token[i+4] = s[0]\n",
    "        all_tokens.append(s[0])\n",
    "\n",
    "    token2id['<pad>'] = PAD_token \n",
    "    token2id['<unk>'] = UNK_token\n",
    "    token2id['SOS'] = SOS_token\n",
    "    token2id['EOS'] = EOS_token\n",
    "    id2token[PAD_token] = '<pad>'\n",
    "    id2token[UNK_token] = '<unk>'\n",
    "    id2token[SOS_token] = 'SOS'\n",
    "    id2token[EOS_token] = 'EOS'\n",
    "    embedding_mat[PAD_token, :] = np.zeros((1,d))\n",
    "    #generate normal dist 1d array for UNK, SOS, EOS token\n",
    "    embedding_mat[UNK_token, :] = np.random.normal(size=d)\n",
    "    embedding_mat[SOS_token, :] = np.random.normal(size=d)\n",
    "    embedding_mat[EOS_token, :] = np.random.normal(size=d)\n",
    "        \n",
    "    return embedding_mat, all_tokens, token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_zh = add + '/zh/zh.vec'\n",
    "fname_eng = add + '/zh/fasttext300d.vec'\n",
    "embedding_mat_zh, all_tokens_zh, token2id_zh, id2token_zh = load_embedding(fname_zh, words_to_load)\n",
    "embedding_mat_en, all_tokens_en, token2id_en, id2token_en = load_embedding(fname_eng, words_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_mat_zh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_mat_en.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Encoder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, direction, layer = 1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.direction = direction\n",
    "        self.layer = layer\n",
    "        \n",
    "        embed_mat = torch.from_numpy(embedding_mat_zh).float()\n",
    "        n, embed_dim = embed_mat.shape\n",
    "        mask = np.zeros((n,1))\n",
    "        mask[0] = 1\n",
    "        mask[1] = 1\n",
    "        mask[2] = 1\n",
    "        mask[3] = 1\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        self.mask_embedding = nn.Embedding.from_pretrained(mask, freeze = False)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed_mat, freeze = True)\n",
    "        \n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, input, input_len, hidden):\n",
    "        # Compute sorted sequence lengths\n",
    "        _, idx_sort = torch.sort(input_len, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        \n",
    "        # get embedding of characters\n",
    "        embed = self.embedding(input)\n",
    "        mask = self.mask_embedding(input)\n",
    "        \n",
    "        embedded = mask*embed + (1-mask)*embed.clone().detach()\n",
    "        \n",
    "        # Sort embedding and length\n",
    "        embedded = embedded.index_select(0, idx_sort)\n",
    "        input_len = input_len.index_select(0, idx_sort)\n",
    "        \n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(embedded, input_len.cpu().numpy(), batch_first=True)\n",
    "        packed_output, hidden = self.gru(packed_emb, hidden)\n",
    "        output, output_lens =  nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Unsort output and last hidden unit\n",
    "        output = output.index_select(0, idx_unsort)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.direction * self.layer, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, hidden_size, kernel_dim, batch_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        embed_mat = torch.from_numpy(embedding_mat_zh).float()\n",
    "        n, embed_dim = embed_mat.shape\n",
    "        mask = np.zeros((n,1))\n",
    "        mask[0] = 1\n",
    "        mask[1] = 1\n",
    "        mask[2] = 1\n",
    "        mask[3] = 1\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        self.mask_embedding = nn.Embedding.from_pretrained(mask, freeze = False)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed_mat, freeze = True)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embed_dim, hidden_size*2, kernel_size=kernel_dim, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size*2, hidden_size*2, kernel_size=kernel_dim, padding=1)\n",
    "        self.linear1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # get embedding of words\n",
    "        embed = self.embedding(input)\n",
    "        mask = self.mask_embedding(input)\n",
    "        \n",
    "        embedded = mask*embed + (1-mask)*embed.clone().detach()\n",
    "        \n",
    "        # perform convolution 1\n",
    "        hidden = self.conv1(embedded.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden)\n",
    "#         hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, hidden.size(1), hidden.size(-1))\n",
    "\n",
    "        # perform convolution 2\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden)\n",
    "#         hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, hidden.size(1), hidden.size(-1))\n",
    "\n",
    "        hidden,_ = hidden.max(dim=1)\n",
    "        out = self.linear1(hidden)\n",
    "        out = out.view(1,out.size(0),out.size(1))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Decoder Without Attention__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        embed_mat = torch.from_numpy(embedding_mat_en).float()\n",
    "        n, embed_dim = embed_mat.shape\n",
    "        mask = np.zeros((n,1))\n",
    "        mask[0] = 1\n",
    "        mask[1] = 1\n",
    "        mask[2] = 1\n",
    "        mask[3] = 1\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        self.mask_embedding = nn.Embedding.from_pretrained(mask, freeze = False)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed_mat, freeze = True)\n",
    "        \n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, word_input, hidden):\n",
    "        # get embedding of words\n",
    "        embed = self.embedding(word_input)\n",
    "        mask = self.mask_embedding(word_input)\n",
    "        \n",
    "        embedded = mask*embed + (1-mask)*embed.clone().detach()\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(1) # B x N\n",
    "        output = self.linear(output)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 3, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, src_len=None):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :param src_len:\n",
    "            used for masking. NoneType or tensor in shape (B) indicating sequence length\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        H = hidden.repeat(max_len,1,1).transpose(0,1)\n",
    "#         encoder_outputs = encoder_outputs.transpose(0,1) # [B*T*H]\n",
    "        attn_energies = self.score(H,encoder_outputs) # compute attention score\n",
    "        \n",
    "        if src_len is not None:\n",
    "            mask = []\n",
    "            for b in range(src_len.size(0)):\n",
    "                mask.append([0] * src_len[b].item() + [1] * (encoder_outputs.size(1) - src_len[b].item()))\n",
    "            mask = cuda_(torch.ByteTensor(mask).unsqueeze(1)) # [B,1,T]\n",
    "            attn_energies = attn_energies.masked_fill(mask, -1e18)\n",
    "        \n",
    "        return F.softmax(attn_energies, dim = 1).unsqueeze(1) # normalize with softmax\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2))) # [B*T*2H]->[B*T*H]\n",
    "        energy = energy.transpose(2,1) # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1) #[B*1*H]\n",
    "        energy = torch.bmm(v,energy) # [B*1*T]\n",
    "        return energy.squeeze(1) #[B*T]\n",
    "\n",
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, embed_size, output_size, dropout_p=0.5):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        # Define layers\n",
    "        \n",
    "        embed_mat = torch.from_numpy(weights_matrix).float()\n",
    "        self.num_embeddings, self.embedding_dim = embed_mat.shape\n",
    "        mask = np.zeros((self.num_embeddings,1))\n",
    "        mask[0] = 1\n",
    "        mask[1] = 1\n",
    "        mask[2] = 1\n",
    "        mask[3] = 1\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        self.mask_embedding = nn.Embedding.from_pretrained(mask, freeze = False)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed_mat, freeze = True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2 + self.embedding_dim, hidden_size, bidirectional = True, dropout=dropout_p)\n",
    "        #self.attn_combine = nn.Linear(hidden_size + embed_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size*2, self.output_size)\n",
    "\n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        '''\n",
    "        :param word_input:\n",
    "            word input for current time step, in shape (B)\n",
    "        :param last_hidden:\n",
    "            last hidden stat of the decoder, in shape (layers*direction*B*H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs in shape (T*B*H)\n",
    "        :return\n",
    "            decoder output\n",
    "        Note: we run this one step at a time i.e. you should use a outer loop \n",
    "            to process the whole sequence\n",
    "        Tip(update):\n",
    "        EncoderRNN may be bidirectional or have multiple layers, so the shape of hidden states can be \n",
    "        different from that of DecoderRNN\n",
    "        You may have to manually guarantee that they have the same dimension outside this function,\n",
    "        e.g, select the encoder hidden state of the foward/backward pass.\n",
    "        '''\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embed = self.embedding(word_input)\n",
    "        mask = self.mask_embedding(word_input)\n",
    "        \n",
    "        word_embedded = mask*embed + (1-mask)*embed.clone().detach()\n",
    "        word_embedded = self.embedding(word_input).view(word_input.size(0), 1, -1) # (1,B,V)\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs)  # (B,1,V)\n",
    "#         context = context.transpose(0, 1)  # (1,B,V)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        \n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        #rnn_input = self.attn_combine(rnn_input) # use it in case your size of rnn_input is different\n",
    "        output, hidden = self.gru(rnn_input.transpose(0, 1), last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,V)->(B,V)\n",
    "        # context = context.squeeze(0)\n",
    "        # update: \"context\" input before final layer can be problematic.\n",
    "        # output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        output = F.log_softmax(self.out(output), dim = 1)\n",
    "        # Return final output, hidden state\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input, target, input_len, target_len, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH, teach_forcing_ratio=0.5, encoder_cnn = False):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    max_input_len = max(input_len)\n",
    "    max_target_len = max(target_len)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    if not encoder_cnn:\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_output, encoder_hidden = encoder(input, input_len, encoder_hidden)\n",
    "    else:\n",
    "        encoder_hidden = encoder(input)\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]]*batch_size, device=device)\n",
    "    decoder_hidden = torch.cat([encoder_hidden[0, :, :].unsqueeze(0), encoder_hidden[1, :, :].unsqueeze(0)], dim = 0)\n",
    "    print(encoder_hidden[0, :, :].unsqueeze(0).size())\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(max_target_len):\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            loss += criterion(decoder_output, target[:,di])\n",
    "            decoder_input = target[:,di].unsqueeze(1)  # Teacher forcing (batch_size, 1)\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(max_target_len):\n",
    "            decoder_output, decoder_hidden, attn_weights= decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(1)  # detach from history as input\n",
    "            loss += criterion(decoder_output, target[:,di])\n",
    "    #         if decoder_input.item() == EOS_token:\n",
    "    #             break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / float(max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input, input_len, encoder_cnn, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param input: string, input sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        max_input_len = max(input_len)\n",
    "        \n",
    "        if not encoder_cnn:\n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_output, encoder_hidden = encoder(input, input_len, encoder_hidden)\n",
    "        else:\n",
    "            encoder_hidden = encoder(input)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]]*batch_size, device=device)\n",
    "        # decode the context vector\n",
    "        decoder_hidden = encoder_hidden # decoder starts from the last encoding sentence\n",
    "        \n",
    "        # output of this function\n",
    "        decoded_words = []\n",
    "#         decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "            decoder_output, decoder_hidden, attention_weights = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(topi.cpu().numpy())\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(1)  # detach from history as input\n",
    "\n",
    "        return np.asarray(decoded_words).T#, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(loader, encoder, decoder, n_iters, encoder_cnn, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    best_bleu = None\n",
    "    save_path = os.getcwd() + '/saved_model/Seq2seq_Attention.pt'\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (input, input_len, target, target_len) in enumerate(train_loader):\n",
    "            loss = train(input, target, input_len, target_len, encoder, decoder, \n",
    "                         encoder_optimizer, decoder_optimizer, criterion, \n",
    "                         max_length=MAX_LENGTH, teach_forcing_ratio=teacher_forcing_ratio, encoder_cnn = encoder_cnn)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            \n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                current_bleu = test(encoder, decoder, val_loader, encoder_cnn)\n",
    "                if not best_bleu or current_bleu > best_bleu:\n",
    "                    torch.save({\n",
    "                                'epoch': iter,\n",
    "                                'encoder_state_dict': encoder.state_dict(),\n",
    "                                'decoder_state_dict': decoder.state_dict(),\n",
    "                                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                                'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "                                'train_loss': loss,\n",
    "                                'best_BLEU': best_bleu\n",
    "                                }, save_path)\n",
    "                    best_bleu = current_bleu\n",
    "                \n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0  \n",
    "                print('%s (Epoch: %d %d%%) | Train Loss: %.4f | Best Bleu: %.4f | Current Blue: %.4f' \n",
    "                      % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg, best_bleu, current_bleu))\n",
    "                with open('bi-gru-attn_result.txt', 'a') as f:\n",
    "                    f.write('%s (Epoch: %d %d%%) | Train Loss: %.4f | Best Bleu: %.4f | Current Blue: %.4f' \n",
    "                          % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg, best_bleu, current_bleu))\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "            torch.cuda.empty_cache()\n",
    "#     showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CuDNN error: CUDNN_STATUS_SUCCESS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ebab2754c9cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#encoder = EncoderCNN(hidden_size, kernel_dim=3, batch_size=batch_size).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# noattn_decoder = DecoderRNN(hidden_size, train_output_lang.n_words).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#weights_matrix, hidden_size, embed_size, output_size, n_layers=1, dropout_p=0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env_py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env_py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env_py3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env_py3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mweight_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_stride0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cudnn_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                     self.batch_first, bool(self.bidirectional))\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_buf_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CuDNN error: CUDNN_STATUS_SUCCESS"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(train_input_lang.n_words, hidden_size, 2, 1).to(device)\n",
    "#encoder = EncoderCNN(hidden_size, kernel_dim=3, batch_size=batch_size).to(device)\n",
    "# noattn_decoder = DecoderRNN(hidden_size, train_output_lang.n_words).to(device)\n",
    "# attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "#weights_matrix, hidden_size, embed_size, output_size, n_layers=1, dropout_p=0.1\n",
    "attn_decoder = BahdanauAttnDecoderRNN(embedding_mat_en, hidden_size, 300, train_output_lang.n_words, dropout_p=0.4).to(device)\n",
    "# f = open('result.txt','w')\n",
    "#UNCOMMENT TO TRAIN THE MODEL\n",
    "trainIters(train_loader, encoder, attn_decoder, n_iters=10, encoder_cnn=False, print_every=100, plot_every=1, learning_rate=LR_RATE)\n",
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "\n",
    "# encoder.load_state_dict(torch.load(\"encoder.pth\"))\n",
    "# attn_decoder1.load_state_dict(torch.load(\"attn_decoder.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(encoder, decoder, data_loader, encoder_cnn):\n",
    "#     total_score = 0\n",
    "#     count = 0\n",
    "    \n",
    "#     candidate_corpus = []\n",
    "#     reference_corpus = []\n",
    "\n",
    "#     for i, (input, input_len, target, target_len) in enumerate(data_loader):\n",
    "#         decoded_words = evaluate(encoder, decoder, input, input_len, encoder_cnn)\n",
    "#         candidate_sentences = []\n",
    "#         for ind in range(decoded_words.shape[1]):\n",
    "#             sent_words = []\n",
    "#             for token in decoded_words[0][ind]:\n",
    "#                 if token != PAD_token and token != EOS_token:\n",
    "# #                     pdb.set_trace()\n",
    "#                     sent_words.append(train_output_lang.index2word[token])\n",
    "#                 else:\n",
    "#                     break\n",
    "#             sent_words = ' '.join(sent_words)\n",
    "#             if count == 0:\n",
    "#                 print('predict: '+sent_words)\n",
    "#                 count += 1\n",
    "#     #             sent_words = ' '.join([train_output_lang.index2word[token] for token in decoded_words[0][ind]])\n",
    "#             candidate_sentences.append(sent_words)\n",
    "#         candidate_corpus.extend(candidate_sentences)\n",
    "\n",
    "#         reference_sentences = []\n",
    "#         for sent in target:\n",
    "#             sent_words = []\n",
    "#             for token in sent:\n",
    "#                 if token.item() != EOS_token:\n",
    "#                     sent_words.append(train_output_lang.index2word[token.item()])\n",
    "#                 else:\n",
    "#                     break\n",
    "#             sent_words = ' '.join(sent_words)\n",
    "#             if count == 1:\n",
    "#                 print('target: '+sent_words)\n",
    "#                 count += 1\n",
    "#     #             sent_words = ' '.join([train_output_lang.index2word[token.item()] for token in sent])\n",
    "#             reference_sentences.append(sent_words)\n",
    "#         reference_corpus.extend(reference_sentences)\n",
    "    \n",
    "#     score = corpus_bleu(candidate_corpus, [reference_corpus], smooth='exp', smooth_floor=0.0, force=False).score\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, data_loader, encoder_cnn):\n",
    "    total_score = 0\n",
    "    count = 0\n",
    "    check = 0\n",
    "    \n",
    "    candidate_corpus = []\n",
    "    reference_corpus = []\n",
    "\n",
    "    for i, (input, input_len, target, target_len) in enumerate(data_loader):\n",
    "        decoded_words = evaluate(encoder, decoder, input, input_len, encoder_cnn)\n",
    "        candidate_sentences = []\n",
    "        for ind in range(decoded_words.shape[1]):\n",
    "            sent_words = []\n",
    "            for token in decoded_words[0][ind]:\n",
    "                if token != PAD_token and token != EOS_token:\n",
    "                    sent_words.append(train_output_lang.index2word[token])\n",
    "                else:\n",
    "                    break\n",
    "            sent_words = ' '.join(sent_words)\n",
    "            if check == 0:\n",
    "                print('predict: '+sent_words)\n",
    "                check += 1\n",
    "            candidate_sentences.append(sent_words)\n",
    "\n",
    "        reference_sentences = []\n",
    "        for sent in target:\n",
    "            sent_words = []\n",
    "            for token in sent:\n",
    "                if token.item() != EOS_token:\n",
    "                    sent_words.append(train_output_lang.index2word[token.item()])\n",
    "                else:\n",
    "                    break\n",
    "            sent_words = ' '.join(sent_words)\n",
    "            if check == 1:\n",
    "                print('target: '+sent_words)\n",
    "                check += 1\n",
    "            reference_sentences.append(sent_words)\n",
    "        count += 1\n",
    "        score = corpus_bleu(candidate_sentences, [reference_sentences], smooth='exp', smooth_floor=0.0, force=False).score\n",
    "        total_score += score\n",
    "    return total_score / float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wz1218/.conda/envs/env_py3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(train_input_lang.n_words, hidden_size, 2, 1).to(device)\n",
    "attn_decoder = BahdanauAttnDecoderRNN(embedding_mat_en, hidden_size, 300, train_output_lang.n_words, dropout_p=0.4).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('mytraining.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EncoderRNN:\n\tMissing key(s) in state_dict: \"gru.weight_ih_l0_reverse\", \"gru.weight_hh_l0_reverse\", \"gru.bias_ih_l0_reverse\", \"gru.bias_hh_l0_reverse\". \n\tsize mismatch for gru.weight_ih_l0: copying a param of torch.Size([300, 300]) from checkpoint, where the shape is torch.Size([768, 300]) in current model.\n\tsize mismatch for gru.weight_hh_l0: copying a param of torch.Size([300, 100]) from checkpoint, where the shape is torch.Size([768, 256]) in current model.\n\tsize mismatch for gru.bias_ih_l0: copying a param of torch.Size([300]) from checkpoint, where the shape is torch.Size([768]) in current model.\n\tsize mismatch for gru.bias_hh_l0: copying a param of torch.Size([300]) from checkpoint, where the shape is torch.Size([768]) in current model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8f3f4d96bf66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/saved_model/Seq2seq_Attention.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env_py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EncoderRNN:\n\tMissing key(s) in state_dict: \"gru.weight_ih_l0_reverse\", \"gru.weight_hh_l0_reverse\", \"gru.bias_ih_l0_reverse\", \"gru.bias_hh_l0_reverse\". \n\tsize mismatch for gru.weight_ih_l0: copying a param of torch.Size([300, 300]) from checkpoint, where the shape is torch.Size([768, 300]) in current model.\n\tsize mismatch for gru.weight_hh_l0: copying a param of torch.Size([300, 100]) from checkpoint, where the shape is torch.Size([768, 256]) in current model.\n\tsize mismatch for gru.bias_ih_l0: copying a param of torch.Size([300]) from checkpoint, where the shape is torch.Size([768]) in current model.\n\tsize mismatch for gru.bias_hh_l0: copying a param of torch.Size([300]) from checkpoint, where the shape is torch.Size([768]) in current model."
     ]
    }
   ],
   "source": [
    "cpt = os.getcwd() + '/saved_model/Seq2seq_Attention.pt'\n",
    "m_dict = torch.load(cpt)\n",
    "encoder.load_state_dict(m_dict['encoder_state_dict'])\n",
    "attn_decoder.load_state_dict(m_dict['decoder_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\n",
      "encoder_state_dict\n",
      "decoder_state_dict\n",
      "encoder_optimizer_state_dict\n",
      "decoder_optimizer_state_dict\n",
      "train_loss\n",
      "best_BLEU\n"
     ]
    }
   ],
   "source": [
    "for i in m_dict:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
