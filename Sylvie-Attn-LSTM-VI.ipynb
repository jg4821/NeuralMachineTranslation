{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import io\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "import itertools\n",
    "import glob\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "from sacrebleu import corpus_bleu\n",
    "import sacrebleu\n",
    "import pdb\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "words_to_load = 100000\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "UNK_token = 3\n",
    "LR_RATE = 0.001\n",
    "MAX_LENGTH = 40\n",
    "hidden_size = 300\n",
    "teacher_forcing_ratio = 0.5\n",
    "save_path = os.getcwd() + '/saved_model/VI En-RNN-De-Attn_LSTM|Hidden-{}|LR-{}|TF-{}|MaxLen-{}'.format(hidden_size,\n",
    "                                                                                                 LR_RATE,\n",
    "                                                                                                 teacher_forcing_ratio,\n",
    "                                                                                                 MAX_LENGTH)\n",
    "train_loss_save_path = os.getcwd() + '/train_loss/VI TrainLoss|En-RNN-De-Attn_LSTM|Hidden-{}|LR-{}|TF-{}|MaxLen-{}'.format(hidden_size,\n",
    "                                                                                                 LR_RATE,\n",
    "                                                                                                 teacher_forcing_ratio,\n",
    "                                                                                                 MAX_LENGTH)\n",
    "bleu_save_path = os.getcwd() + '/bleu/VI BLEU|En-RNN-De-Attn_LSTM|Hidden-{}|LR-{}|TF-{}|MaxLen-{}'.format(hidden_size,\n",
    "                                                                                                 LR_RATE,\n",
    "                                                                                                 teacher_forcing_ratio,\n",
    "                                                                                                 MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocess Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"<pad>\", 3: \"<unk>\"}\n",
    "        self.n_words = 4  # Count SOS, EOS, pad and unk\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "# def normalizeZh(s):\n",
    "#     s = s.strip()\n",
    "#     s = re.sub(\"\\s+\", \" \", s)\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filterPair(p):\n",
    "#     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "#         len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# def filterPairs(pairs):\n",
    "#     return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    filtered = []\n",
    "    for i in p:\n",
    "        filtered.append(' '.join(i.split(' ')[:MAX_LENGTH-1]))\n",
    "    return filtered\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [filterPair(pair) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(dataset, lang1, lang2):\n",
    "    vietnamese = os.getcwd()+'/iwslt-vi-en/{}.tok.{}'.format(dataset, lang1)\n",
    "    english = os.getcwd()+'/iwslt-vi-en/{}.tok.{}'.format(dataset, lang2)\n",
    "\n",
    "    vietnamese_lines = open(vietnamese, encoding='utf-8').read().strip().split('\\n')\n",
    "    english_lines = open(english, encoding='utf-8').read().strip().split('\\n')\n",
    "    length = len(vietnamese_lines)\n",
    "\n",
    "    pairs = [[vietnamese_lines[i], normalizeString(english_lines[i])] for i in range(length)]\n",
    "    pairs = filterPairs(pairs)\n",
    "    \n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_lang, train_output_lang, train_pairs = readLangs('train', 'vi', 'en')\n",
    "val_input_lang, val_output_lang, val_pairs = readLangs('dev', 'vi', 'en')\n",
    "test_input_lang, test_output_lang, test_pairs = readLangs('test', 'vi', 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(ft_path, words_to_load):\n",
    "    fin = io.open(ft_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    vocab_size = words_to_load + 4\n",
    "    embedding_dim = d\n",
    "\n",
    "    embedding_mat = np.zeros((vocab_size, embedding_dim))\n",
    "    token2id = {}\n",
    "    id2token = {}\n",
    "    all_tokens = ['SOS', 'EOS', '<unk>', '<pad>']\n",
    "\n",
    "    for i, line in enumerate(fin):\n",
    "        if i >= words_to_load:\n",
    "            break\n",
    "        s = line.rstrip().split(' ')\n",
    "        embedding_mat[i+4, :] = np.asarray(s[1:])\n",
    "        token2id[s[0]] = i+4\n",
    "        id2token[i+4] = s[0]\n",
    "        all_tokens.append(s[0])\n",
    "\n",
    "    token2id['<pad>'] = PAD_token \n",
    "    token2id['<unk>'] = UNK_token\n",
    "    token2id['SOS'] = SOS_token\n",
    "    token2id['EOS'] = EOS_token\n",
    "    id2token[PAD_token] = '<pad>'\n",
    "    id2token[UNK_token] = '<unk>'\n",
    "    id2token[SOS_token] = 'SOS'\n",
    "    id2token[EOS_token] = 'EOS'\n",
    "    embedding_mat[PAD_token, :] = np.zeros((1,d))\n",
    "    #generate normal dist 1d array for UNK, SOS, EOS token\n",
    "    embedding_mat[UNK_token, :] = np.random.normal(size=d)\n",
    "    embedding_mat[SOS_token, :] = np.random.normal(size=d)\n",
    "    embedding_mat[EOS_token, :] = np.random.normal(size=d)\n",
    "        \n",
    "    return embedding_mat, all_tokens, token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_vi = os.getcwd()+'/Embedding/wiki.vi.vec'\n",
    "fname_eng = os.getcwd()+'/Embedding/wiki-news-300d-1M.vec'\n",
    "embedding_mat_vi, all_tokens_vi, token2id_vi, id2token_vi = load_embedding(fname_vi, words_to_load)\n",
    "embedding_mat_en, all_tokens_en, token2id_en, id2token_en = load_embedding(fname_eng, words_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Loader__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_lang, output_lang, pairs):\n",
    "        \"\"\"\n",
    "        @param data_list_1: list of sentence 1 tokens \n",
    "        @param data_list_2: list of sentence 2 tokens\n",
    "        @param target_list: list of review targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.input_w2i = input_lang\n",
    "        self.output_w2i = output_lang\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        input_sentence = self.pairs[key][0]\n",
    "        input_indexes = [self.input_w2i[word] if word in self.input_w2i else UNK_token for word in input_sentence.split(' ')]\n",
    "        input_indexes.append(EOS_token)\n",
    "        input_length = len(input_indexes)\n",
    "\n",
    "        output_sentence = self.pairs[key][1]\n",
    "        output_indexes = [self.output_w2i[word] if word in self.output_w2i else UNK_token for word in output_sentence.split(' ')]\n",
    "        output_indexes.append(EOS_token)\n",
    "        output_length = len(output_indexes)\n",
    "        return [input_indexes, input_length, output_indexes, output_length]\n",
    "\n",
    "    \n",
    "def NMTDataset_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    input_ls = []\n",
    "    output_ls = []\n",
    "    input_length_ls = []\n",
    "    output_length_ls = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        input_length_ls.append(datum[1])\n",
    "        output_length_ls.append(datum[3])\n",
    "    \n",
    "    #find max length in each batch\n",
    "    max_input = sorted(input_length_ls)[-1]\n",
    "    max_output = sorted(output_length_ls)[-1]\n",
    "    \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_input = np.pad(np.array(datum[0]), \n",
    "                                  pad_width=((0,MAX_LENGTH-datum[1])), \n",
    "                                  mode=\"constant\", constant_values=2).tolist()\n",
    "        padded_vec_output = np.pad(np.array(datum[2]), \n",
    "                                   pad_width=((0,MAX_LENGTH-datum[3])), \n",
    "                                   mode=\"constant\", constant_values=2).tolist()\n",
    "        input_ls.append(padded_vec_input)\n",
    "        output_ls.append(padded_vec_output)\n",
    "    return [torch.tensor(torch.from_numpy(np.array(input_ls)), device=device), \n",
    "            torch.tensor(input_length_ls, device=device), \n",
    "            torch.tensor(torch.from_numpy(np.array(output_ls)), device=device), \n",
    "            torch.tensor(output_length_ls, device=device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataloader\n",
    "train_dataset = NMTDataset(token2id_vi, token2id_en, train_pairs)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=NMTDataset_collate_func,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "val_dataset = NMTDataset(token2id_vi, token2id_en, val_pairs)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         collate_fn=NMTDataset_collate_func,\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Encoder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        embed_mat = torch.from_numpy(embedding_mat_vi).float()\n",
    "        n, embed_dim = embed_mat.shape\n",
    "#         mask = np.zeros((n,1))\n",
    "#         mask[0] = 1\n",
    "#         mask[1] = 1\n",
    "#         mask[2] = 1\n",
    "#         mask[3] = 1\n",
    "#         mask = torch.from_numpy(mask).float()\n",
    "#         self.mask_embedding = nn.Embedding.from_pretrained(mask, freeze = False)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed_mat, freeze = True)\n",
    "        \n",
    "#         self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, input, input_len, hidden):        \n",
    "        # get embedding of characters\n",
    "        embed = self.embedding(input)\n",
    "#         mask = self.mask_embedding(input)\n",
    "        \n",
    "#         embedded = mask*embed + (1-mask)*embed.clone().detach()\n",
    "        embedded = embed\n",
    "#         output, hidden = self.gru(embedded, hidden)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return (torch.zeros(2, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(2, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Decoder With Attention__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        embed_mat = torch.from_numpy(embedding_mat_vi).float()\n",
    "        n, embed_dim = embed_mat.shape\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embed_mat, freeze=True)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "#         self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embed = self.embedding(input)\n",
    "        embed = self.dropout(embed)   \n",
    "        \n",
    "        hidden_reshaped = [h.view(hidden[0].size()[1],1,-1) for h in hidden]\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embed, hidden_reshaped[0]), 2)), dim=2)\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "#         print(attn_weights.size())\n",
    "#         print(encoder_outputs.size())\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs).squeeze(1)\n",
    "        \n",
    "        output = torch.cat((embed.squeeze(1), attn_applied), 1)\n",
    " \n",
    "        output = self.attn_combine(output).unsqueeze(1)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        hidden_reshaped = [h.view(1,h.size()[0],-1) for h in hidden_reshaped]\n",
    "        output, hidden = self.lstm(output, hidden_reshaped)\n",
    "        output = self.softmax(self.out(output.squeeze(1)))\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return torch.randn(self.num_layers, batch_size, self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input, target, input_len, target_len, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teach_forcing_ratio=0.5, encoder_cnn = False):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    max_input_len = max(input_len)\n",
    "    max_target_len = max(target_len)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    if not encoder_cnn:\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_output, encoder_hidden = encoder(input, input_len, encoder_hidden)\n",
    "#         print(encoder_output.size())\n",
    "    else:\n",
    "        encoder_hidden = encoder(input)\n",
    "        \n",
    "    decoder_input = torch.tensor([[SOS_token]]*batch_size, device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(max_target_len):\n",
    "#             decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            loss += criterion(decoder_output, target[:,di])\n",
    "            decoder_input = target[:,di].unsqueeze(1)  # Teacher forcing (batch_size, 1)\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(max_target_len):\n",
    "#             decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            loss += criterion(decoder_output, target[:,di])\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(1)  # detach from history as input\n",
    "    #         if decoder_input.item() == EOS_token:\n",
    "    #             break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / float(max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(loader, encoder, decoder, n_iters, encoder_cnn, save_path, print_every=1000, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    best_bleu = None\n",
    "#     save_path = os.getcwd() + '/saved_model/En-CNN-De-NoAttn|Hidden-{}|LR-{}|TF-{}|MaxLen-{}.pt'.format(hidden_size, \n",
    "#                                                                                                         learning_rate,\n",
    "#                                                                                                         teacher_forcing_ratio,\n",
    "#                                                                                                         max_length)\n",
    "    save_path = save_path + '.pt'\n",
    "    \n",
    "    train_loss_hist = []\n",
    "    bleu_hist = []\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (input, input_len, target, target_len) in enumerate(train_loader):\n",
    "            loss = train(input, target, input_len, target_len, encoder, decoder, \n",
    "                         encoder_optimizer, decoder_optimizer, criterion, \n",
    "                         teach_forcing_ratio=teacher_forcing_ratio, encoder_cnn = encoder_cnn)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            \n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                current_bleu = test(encoder, decoder, val_loader, encoder_cnn)\n",
    "                if not best_bleu or current_bleu > best_bleu:\n",
    "                    torch.save({\n",
    "                                'epoch': iter,\n",
    "                                'encoder_state_dict': encoder.state_dict(),\n",
    "                                'decoder_state_dict': decoder.state_dict(),\n",
    "                                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                                'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "                                'train_loss': loss,\n",
    "                                'best_BLEU': best_bleu\n",
    "                                }, save_path)\n",
    "                    best_bleu = current_bleu\n",
    "                \n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                train_loss_hist.append(print_loss_avg)\n",
    "                bleu_hist.append(current_bleu)\n",
    "                print('%s (Epoch: %d %d%%) | Train Loss: %.4f | Best Bleu: %.4f | Current Blue: %.4f' \n",
    "                      % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg, best_bleu, current_bleu))\n",
    "\n",
    "#             if i % plot_every == 0:\n",
    "#                 plot_loss_avg = plot_loss_total / plot_every\n",
    "#                 plot_losses.append(plot_loss_avg)\n",
    "#                 plot_loss_total = 0\n",
    "#     showPlot(plot_losses)\n",
    "    return train_loss_hist, bleu_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input, input_len, encoder_cnn, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param input: string, input sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        max_input_len = max(input_len)\n",
    "        \n",
    "        if not encoder_cnn:\n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_output, encoder_hidden = encoder(input, input_len, encoder_hidden)\n",
    "        else:\n",
    "            encoder_hidden = encoder(input)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]]*batch_size, device=device)\n",
    "        # decode the context vector\n",
    "        decoder_hidden = encoder_hidden # decoder starts from the last encoding sentence\n",
    "        \n",
    "        # output of this function\n",
    "        decoded_words = []\n",
    "#         decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(topi.cpu().numpy())\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(1)  # detach from history as input\n",
    "\n",
    "        return np.asarray(decoded_words).T#, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, data_loader, encoder_cnn):\n",
    "    total_score = 0\n",
    "    count = 0\n",
    "    \n",
    "    candidate_corpus = []\n",
    "    reference_corpus = []\n",
    "\n",
    "    for i, (input, input_len, target, target_len) in enumerate(data_loader):\n",
    "        decoded_words = evaluate(encoder, decoder, input, input_len, encoder_cnn)\n",
    "        candidate_sentences = []\n",
    "        for ind in range(decoded_words.shape[1]):\n",
    "            sent_words = []\n",
    "            for token in decoded_words[0][ind]:\n",
    "                if token != PAD_token and token != EOS_token:\n",
    "#                     pdb.set_trace()\n",
    "                    sent_words.append(id2token_en[token.item()])\n",
    "#                     sent_words.append(train_output_lang.index2word[token])\n",
    "                else:\n",
    "                    break\n",
    "            sent_words = ' '.join(sent_words)\n",
    "            if count == 0:\n",
    "                print('predict: '+sent_words)\n",
    "                count += 1\n",
    "    #             sent_words = ' '.join([train_output_lang.index2word[token] for token in decoded_words[0][ind]])\n",
    "            candidate_sentences.append(sent_words)\n",
    "        candidate_corpus.extend(candidate_sentences)\n",
    "\n",
    "        reference_sentences = []\n",
    "        for sent in target:\n",
    "            sent_words = []\n",
    "            for token in sent:\n",
    "                if token.item() != EOS_token:\n",
    "                    sent_words.append(id2token_en[token.item()])\n",
    "#                     sent_words.append(train_output_lang.index2word[token.item()])\n",
    "                else:\n",
    "                    break\n",
    "            sent_words = ' '.join(sent_words)\n",
    "            if count == 1:\n",
    "                print('target: '+sent_words)\n",
    "                count += 1\n",
    "    #             sent_words = ' '.join([train_output_lang.index2word[token.item()] for token in sent])\n",
    "            reference_sentences.append(sent_words)\n",
    "        reference_corpus.extend(reference_sentences)\n",
    "    \n",
    "    score = corpus_bleu(candidate_corpus, [reference_corpus], smooth='exp', smooth_floor=0.0, force=False).score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: indica proficiency accessory accessory\n",
      "target: i joined forces with many other <unk> inside and outside <unk> to call for a day of rage and to initiate a revolution against the tyrannical regime of <unk> .\n",
      "0m 14s (- 4m 26s) (Epoch: 1 5%) | Train Loss: 0.0115 | Best Bleu: 0.0013 | Current Blue: 0.0013\n",
      "predict: and <unk> s <unk> s the <unk> s the <unk> s the <unk> <unk> .\n",
      "target: we have never met a single human being in the world who can make it sell it and look after the money .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12m 1s (- 228m 35s) (Epoch: 1 5%) | Train Loss: 3.2727 | Best Bleu: 5.1695 | Current Blue: 5.1695\n",
      "predict: and <unk> <unk> <unk> . . .\n",
      "target: some i even considered like my second home .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23m 57s (- 455m 4s) (Epoch: 1 5%) | Train Loss: 2.9352 | Best Bleu: 5.3165 | Current Blue: 5.3165\n",
      "predict: and <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <unk> <unk>\n",
      "target: so so the government says <unk> do it again . <unk> <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35m 47s (- 680m 2s) (Epoch: 1 5%) | Train Loss: 2.8111 | Best Bleu: 5.9015 | Current Blue: 5.9015\n",
      "predict: and we <unk> <unk> to to to to the the the the the <unk> <unk> <unk> .\n",
      "target: there was no question that his children would receive an education including his daughters despite the taliban despite the risks .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47m 39s (- 905m 36s) (Epoch: 1 5%) | Train Loss: 2.7478 | Best Bleu: 6.2177 | Current Blue: 6.2177\n",
      "predict: it <unk> s a . . . .\n",
      "target: thanks so much . max little everybody .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49m 46s (- 447m 59s) (Epoch: 2 10%) | Train Loss: 0.4475 | Best Bleu: 6.2177 | Current Blue: 6.1929\n",
      "predict: and the it <unk> the to to to to to to . .\n",
      "target: <unk> this world will be much poorer without these wonderful species .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61m 39s (- 554m 52s) (Epoch: 2 10%) | Train Loss: 2.6364 | Best Bleu: 7.0160 | Current Blue: 7.0160\n",
      "predict: and is the the and and and and and and <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "target: they were lawyers journalists priests they all said <unk> we don <unk> t want this . <unk> <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73m 29s (- 661m 21s) (Epoch: 2 10%) | Train Loss: 2.6091 | Best Bleu: 7.0206 | Current Blue: 7.0206\n",
      "predict: we <unk> to to to . . .\n",
      "target: we have helped to start businesses .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85m 16s (- 767m 31s) (Epoch: 2 10%) | Train Loss: 2.5838 | Best Bleu: 7.0206 | Current Blue: 6.8971\n",
      "predict: and i i to to to people the of of the . . .\n",
      "target: patronizing i treat everybody from another culture as if they were my servants .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97m 4s (- 873m 39s) (Epoch: 2 10%) | Train Loss: 2.5304 | Best Bleu: 7.6110 | Current Blue: 7.6110\n",
      "predict: this is s in the in i i i a a a a a . .\n",
      "target: this is six months of my life into this file .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99m 9s (- 561m 56s) (Epoch: 3 15%) | Train Loss: 0.4175 | Best Bleu: 7.6110 | Current Blue: 7.4320\n",
      "predict: and if i <unk> m a a to to and and and and and and you you to you\n",
      "target: and if i could communicate just one thing to <unk> and to sam and to you it would be that you don <unk> t have to be normal .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110m 55s (- 628m 37s) (Epoch: 3 15%) | Train Loss: 2.4545 | Best Bleu: 8.4655 | Current Blue: 8.4655\n",
      "predict: and i have a . .\n",
      "target: so i had an idea .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122m 41s (- 695m 16s) (Epoch: 3 15%) | Train Loss: 2.4387 | Best Bleu: 8.4655 | Current Blue: 7.7597\n",
      "predict: and <unk> you <unk> re going to it it <unk> <unk> <unk> .\n",
      "target: and now you <unk> re of course curious if it also worked .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134m 29s (- 762m 7s) (Epoch: 3 15%) | Train Loss: 2.4298 | Best Bleu: 8.4655 | Current Blue: 8.0117\n",
      "predict: and the the i i i i i i <unk> a a of the the the the the the the the the\n",
      "target: in the <unk> i found children carrying stone for miles down mountainous terrain to trucks waiting at roads below .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146m 15s (- 828m 46s) (Epoch: 3 15%) | Train Loss: 2.4148 | Best Bleu: 8.4655 | Current Blue: 8.0003\n"
     ]
    }
   ],
   "source": [
    "encoder_hidden_size = int(hidden_size/2)\n",
    "encoder = EncoderRNN(hidden_size = encoder_hidden_size).to(device)\n",
    "# encoder = EncoderCNN(hidden_size,kernel_dim=3,batch_size=batch_size).to(device)\n",
    "# noattn_decoder = DecoderRNN(hidden_size, train_output_lang.n_words).to(device)\n",
    "# noattn_decoder = DecoderRNN(hidden_size, embedding_mat_en.shape[0]).to(device)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size, embedding_mat_vi.shape[0]).to(device)\n",
    "\n",
    "#UNCOMMENT TO TRAIN THE MODEL\n",
    "train_loss_hist, bleu_hist = trainIters(train_loader, encoder, attn_decoder, n_iters=20, encoder_cnn=False, save_path = save_path, print_every=1000, learning_rate=LR_RATE)\n",
    "\n",
    "with open(train_loss_save_path, 'wb') as f:\n",
    "     pickle.dump(train_loss_hist, f)\n",
    "with open(bleu_save_path, 'wb') as f:\n",
    "     pickle.dump(bleu_hist, f)\n",
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "\n",
    "# encoder.load_state_dict(torch.load(\"encoder.pth\"))\n",
    "# attn_decoder1.load_state_dict(torch.load(\"attn_decoder.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_loss_save_path, 'rb') as f:\n",
    "     train_loss_hist_pk = pickle.load(f)\n",
    "with open(bleu_save_path, 'rb') as f:\n",
    "     bleu_hist_pk = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
