{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_Itoken = 2\n",
    "UNK_token = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"<pad>\", 3: \"<unk>\"}\n",
    "        self.n_words = 4  # Count SOS, EOS, pad and unk\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(dataset, lang1, lang2):\n",
    "    chinese = os.getcwd()+'/iwslt-zh-en/{}.tok.{}'.format(dataset, lang1)\n",
    "    english = os.getcwd()+'/iwslt-zh-en/{}.tok.{}'.format(dataset, lang2)\n",
    "\n",
    "    chinese_lines = open(chinese, encoding='utf-8').read().strip().split('\\n')\n",
    "    english_lines = open(english, encoding='utf-8').read().strip().split('\\n')\n",
    "    length = len(chinese_lines)\n",
    "\n",
    "    pairs = [[chinese_lines[i], normalizeString(english_lines[i])] for i in range(length)]\n",
    "    \n",
    "    input_lang = Lang(chinese_lines)\n",
    "    output_lang = Lang(english_lines)\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_lang, train_output_lang, train_pairs = readLangs('train', 'zh', 'en')\n",
    "val_input_lang, val_output_lang, val_pairs = readLangs('dev', 'zh', 'en')\n",
    "test_input_lang, test_output_lang, test_pairs = readLangs('test', 'zh', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_lang, output_lang, pairs):\n",
    "        \"\"\"\n",
    "        @param data_list_1: list of sentence 1 tokens \n",
    "        @param data_list_2: list of sentence 2 tokens\n",
    "        @param target_list: list of review targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.pairs = pairs\n",
    "        assert (len(self.input_lang.name) == len(self.pairs))\n",
    "        assert (len(self.output_lang.name) == len(self.pairs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        input_sentence = self.pairs[key][0]\n",
    "        input_indexes = [self.input_lang.word2index[word] for word in input_sentence.split(' ')]\n",
    "        input_indexes.append(EOS_token)\n",
    "        input_length = len(input_indexes)\n",
    "\n",
    "        output_sentence = self.pairs[key][1]\n",
    "        output_indexes = [self.output_lang.word2index[word] for word in output_sentence.split(' ')]\n",
    "        output_indexes.append(EOS_token)\n",
    "        output_length = len(output_indexes)\n",
    "        return [input_indexes, input_length, output_indexes, output_length]\n",
    "\n",
    "    \n",
    "def NMTDataset_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    input_ls = []\n",
    "    output_ls = []\n",
    "    input_length_ls = []\n",
    "    output_length_ls = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        input_length_ls.append(datum[1])\n",
    "        output_length_ls.append(datum[3])\n",
    "    \n",
    "    #find max length in each batch\n",
    "    max_input = sorted(input_length_ls)[-1]\n",
    "    max_output = sorted(output_length_ls)[-1]\n",
    "    \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_input = np.pad(np.array(datum[0]), \n",
    "                                  pad_width=((0,max_input-datum[1])), \n",
    "                                  mode=\"constant\", constant_values=0).tolist()\n",
    "        padded_vec_output = np.pad(np.array(datum[2]), \n",
    "                                   pad_width=((0,max_output-datum[3])), \n",
    "                                   mode=\"constant\", constant_values=0).tolist()\n",
    "        input_ls.append(padded_vec_input)\n",
    "        output_ls.append(padded_vec_output)\n",
    "    return [torch.tensor(torch.from_numpy(np.array(input_ls)), device=device), \n",
    "            torch.tensor(input_length_ls, device=device), \n",
    "            torch.tensor(torch.from_numpy(np.array(output_ls)), device=device), \n",
    "            torch.tensor(output_length_ls, device=device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataloader\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NMTDataset(train_input_lang, train_output_lang, train_pairs)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=NMTDataset_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NMTDataset(val_input_lang, val_output_lang, val_pairs)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         collate_fn=NMTDataset_collate_func,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   58,    32, 12752,  ...,     0,     0,     0],\n",
       "         [   50,   309, 39721,  ...,     0,     0,     0],\n",
       "         [  111,   284,   175,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  329,  1026,   218,  ...,     0,     0,     0],\n",
       "         [ 6494,  1397,    32,  ...,     0,     0,     0],\n",
       "         [11659, 64970,     6,  ...,     0,     0,     0]], device='cuda:0'),\n",
       " tensor([ 23,  47,  25,   9,  22,  15,  12,  28,  11,  45,  25,  27,   6,   7,\n",
       "           8,  12,  30,  11,  19, 110,  21,  12,  31,  17,  40,  19,  22,  14,\n",
       "           5,   7,  30,  32], device='cuda:0'),\n",
       " tensor([[   44,    38,   150,  ...,     0,     0,     0],\n",
       "         [  179,   503,   179,  ...,     0,     0,     0],\n",
       "         [    5,    76,   212,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 1980,   422,  4117,  ...,     0,     0,     0],\n",
       "         [  695,  5688,    38,  ...,     0,     0,     0],\n",
       "         [10738,  3020,   998,  ...,     0,     0,     0]], device='cuda:0'),\n",
       " tensor([22, 43, 20, 14, 17, 15, 10, 21, 12, 27, 20, 21,  7, 10,  9,  9, 18, 11,\n",
       "         17, 83, 15, 14, 31, 15, 38, 14, 20, 11,  4,  6, 24, 27],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
